# Migration Script Manual Tests

This directory contains end-to-end tests for the migration script. Each object type has its own folder with test configuration.

## Test Approach

The test validates the migration script by:
1. Creating test objects on Snowflake
2. Fetching objects via data source and generating a CSV
3. Running the migration script to generate Terraform code with import blocks
4. Applying the generated code (which imports existing objects)
5. **Verifying the plan is empty** (no changes needed = successful import)

## Quick Start

### Prerequisites

Set account information in environment variables:
- SNOWFLAKE_ACCOUNT
- SNOWFLAKE_ORGANIZATION
- SNOWFLAKE_HOST
- SNOWFLAKE_USER
- SNOWFLAKE_PASSWORD
- SNOWFLAKE_ROLE

### Step 1: Navigate to object type folder

```bash
cd grants  # or schemas, warehouses, users, etc.
```

### Step 2: Clean up any previous state

```bash
rm -rf .terraform terraform.tfstate terraform.tfstate.backup generated_output.tf
```

### Step 3: Initialize Terraform

```bash
terraform init
```

### Step 4: Create test objects AND generate CSV

```bash
terraform apply -auto-approve
```

This creates objects from `objects_def.tf` and generates `objects.csv` via `datasource.tf`.

### Step 5: Run migration script

```bash
go run github.com/Snowflake-Labs/terraform-provider-snowflake/pkg/scripts/migration_script@main -import=block grants < objects.csv > generated_output.tf
```

### Step 6: Apply generated output (imports existing objects)

```bash
terraform apply -auto-approve
```

This imports the existing Snowflake objects into Terraform state.

### Step 7: Verify plan is empty

```bash
terraform plan
```

**Success criteria:** The plan should show "No changes. Your infrastructure matches the configuration."

If there are changes, the migration script generated incorrect values.

### Step 8: Cleanup (when done)

```bash
terraform destroy -auto-approve
```

## Directory Structure

```
manual_tests/
├── README.md                # This file
├── users/                   # Users test
│   ├── objects_def.tf       # Creates test users on Snowflake
│   ├── datasource.tf        # Fetches users, generates CSV
│   ├── objects.csv          # Generated CSV (after terraform apply)
│   └── generated_output.tf  # Generated by migration script
├── schemas/
├── warehouses/
├── grants/
└── <new_object_type>/       # Add new object types here
```

## Adding a New Object Type

To add tests for a new object type (e.g., `warehouses`):

### Step 1: Create the folder

```bash
mkdir -p warehouses
```

### Step 2: Create `objects_def.tf`

Create test objects with various configurations:

```hcl
terraform {
  required_providers {
    snowflake = {
      source = "snowflakedb/snowflake"
    }
  }
}

provider "snowflake" {}

# Basic warehouse
resource "snowflake_warehouse" "basic" {
  name = "MIGRATION_TEST_WH_BASIC"
}

# Warehouse with all parameters
resource "snowflake_warehouse" "complete" {
  name           = "MIGRATION_TEST_WH_COMPLETE"
  comment        = "Test warehouse for migration"
  warehouse_size = "XSMALL"
  # ... more parameters
}
```

**Important naming convention:** Use `MIGRATION_TEST_` prefix for all test objects.

### Step 3: Create `datasource.tf`

Fetch the objects and generate CSV:

```hcl
# Fetch test warehouses
data "snowflake_warehouses" "test_warehouses" {
  like = "MIGRATION_TEST_WH_%"
}

locals {
  # Flatten the data source output
  warehouses_flattened = [
    for wh in data.snowflake_warehouses.test_warehouses.warehouses :
    wh.show_output[0]
  ]

  # Create CSV header
  csv_header = length(local.warehouses_flattened) > 0 ? join(",", [
    for key in keys(local.warehouses_flattened[0]) : "\"${key}\""
  ]) : ""

  # CSV escape function
  csv_escape = length(local.warehouses_flattened) > 0 ? {
    for wh in local.warehouses_flattened :
    wh.name => {
      for key in keys(local.warehouses_flattened[0]) :
      key => replace(
        replace(
          replace(tostring(lookup(wh, key, "")), "\\", "\\\\"),
          "\n", "\\n"
        ),
        "\"", "\"\""
      )
    }
  } : {}

  # Create CSV rows
  csv_rows = length(local.warehouses_flattened) > 0 ? [
    for wh in local.warehouses_flattened :
      join(",", [
        for key in keys(local.warehouses_flattened[0]) :
        "\"${local.csv_escape[wh.name][key]}\""
      ])
  ] : []

  csv_content = join("\n", concat([local.csv_header], local.csv_rows))
}

# Write CSV file
resource "local_file" "csv" {
  content  = local.csv_content
  filename = "${path.module}/objects.csv"
}

# Debug outputs
output "objects_found" {
  value = length(local.warehouses_flattened)
}
```

### Step 4: Test it

```bash
cd warehouses
rm -rf .terraform terraform.tfstate* generated_output.tf
terraform init
terraform apply -auto-approve

cd ..
go run . -import=block warehouses < manual_tests/warehouses/objects.csv > manual_tests/warehouses/generated_output.tf

cd manual_tests/warehouses
terraform apply -auto-approve
terraform plan  # Should show no changes!
```

## CSV Format Notes

The CSV files use proper RFC 4180 escaping:

- **Double quotes** are escaped by doubling: `"` → `""`
- **Backslashes** are escaped: `\` → `\\`
- **Newlines** are converted to literal `\n` for multi-line values (like RSA keys)
- All fields are quoted

The migration script's `csvUnescape` function handles decoding these escape sequences.
